# -*- coding: utf-8 -*-
"""Trainer_Module_transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xOAhpxE8hciLVTUporfZdipxc5Bh06Gr
"""

# !pip install -q evaluate
# !pip install -q nlpaug
# !pip install -q transformers
# !pip install transformers[torch]
# !pip install accelerate -U
# !pip install accelerate
# ! pip install -U accelerate
# ! pip install -U transformers
# import torch, time
# import evaluate
# import numpy as np
# import pandas as pd
# from tqdm import tqdm
# import nlpaug.augmenter.word as naw
# from datasets import Dataset, DatasetDict
# from sklearn.model_selection import train_test_split

# from transformers import AutoTokenizer
# from transformers import AutoModelForSequenceClassification
# from transformers import TrainingArguments, Trainer, TrainerCallback



# Trainer module based on pytorch , hence its manadatory , or we can also use keras to use trainer module
# Trainer module is from training , which aspects model,training set, validation, test dataset, tokenizer, hyperparametre, metrics to use from users etc
# This module returns / used for training , evalution, predictions

# when training transformer using Trainer should not be used when we are not using pytorch , as trainer module is based on pytorch
# If not using pytorch , we can train transformer using keras , hence transformer module not used
# for fine grained control over training  loop , we can use accelerate library

from datasets import load_dataset   # datasets is library having inbuild datasets compatible for transformer library
emotion_df = load_dataset("emotion")  #dataset of emotions , joy,anger etc. Multiclass calssification
emotion_df     # 16000 records for training data , In datasets library every row represented as dictionary , columns as key and there name as values

emotion_df['train'][0]  # first record from train df

emotion_df['train'][15999]  # last record from train df  # text is data and label is column or Y

# Name of variables as "KEY" while records in it act as "values"

emotion_df['test'][0]   # first record of test dataframe

# converting this dictionary format with pandas df
train_df = emotion_df['train'].to_pandas()
train_df.head()

features = emotion_df['train'].features  # df.info() # MultiClass Classifiaction (6)classes
features # for datatypes of columns # its more like df.info() function in pandas

train_df.info()

# which label id , shows which class among 6 classes
features['label'].int2str(5)  #0,1,2,3,4,5

# MAPPING OF EACH LABEL ID TO LABLE NAME  :
# useful when we do text classification using transformer , hence instead of giving output in int
# model return output in "Labels"

id2label = {idx:features['label'].int2str(idx) for idx in range(6)}
print(id2label)
label2id = {j:i for i,j in id2label.items() }
print(label2id)

# Cheacking distribution of classes
train_df['label'].value_counts(normalize=True).sort_index() # Imbalaced dataset
# model will be good at predicting majorily represented classes , and will struggle a lot for underrepresented class like "5:surprise class"
# why not oversampling done while using transformers ?
#>> as deep learning model are very much good at memorisiing or discovring data, hence model may get biased in production for those duplicated class, or model may not generalized very
# well for testing points

#Instead of that we can modify loss function of model during training , hence we can incorporate distribution of classes in loss function so to give higher weights to underrepresented class



#Tokenize Data : Used MiniLm model for tokenize , not bertbase : 109million param ,81%performance slower, miniLM :66M param but still 80% performance faster

from transformers import AutoTokenizer
model_ckpt = "microsoft/MiniLM-L12-H384-uncased"   # model checkpoint
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

tokenizer(emotion_df['train']['text'][:1]) # encoding first record

emotion_df['train']['text'][0]

# Tokenizer to all dataset (train,test, validate)
def tokenize_text(data):
  return tokenizer(data['text'],truncation= True,max_length=512)

emotion_df = emotion_df.map(tokenize_text,batched=True) #map is funtion of dataset library to aply finction to all data at a time
emotion_df



emotion_df

len(train_df)

train_df['label'].value_counts().sort_index()

# Dealing with imbalaced data
class_weights = (1-(train_df['label'].value_counts().sort_index() / len(train_df))).values
class_weights
#calculating class weights based on the distribution of labels in your training dataset.
#These class weights are used to address the issue of imbalanced data when training a machine learning model, particularly in the context of multi-class classification
#  The purpose of subtracting this from 1 is to give more weight to underrepresented classes. In other words, the class weights will be higher for minority classes.
# and hence model does not get too bias in mahority classes

1-(4666/16000)

# as trainer module of transformer is based on pytorch , we ultimately need to convert this from numpy array to torch tensors
import torch
class_weights = torch.from_numpy(class_weights).float().to("cuda")
class_weights

emotion_df

train_df.head()

# we have to tell trainer module which is our target column  : in our case its "label"
# In documentaion its written that :
# "your model can compute the loss if a labels argument is provided and that loss is returned as the first element of the tuple (if your model returns tuples)"
# refremce : https://huggingface.co/docs/transformers/main_classes/trainer

# Hence we should first change the name of our column from "label":"labels"
emotion_df = emotion_df.rename_columns({"label":"labels"})
emotion_df



# Now we can define trainer : here we will define custom trainer as we have to do or incorporate  custom loss computation for our imbalanced dataset
import torch
from torch import nn
from transformers import Trainer


class WeightedLossTrainer(Trainer):  # weighted Loss trainer will be our new class , which is subclass in Trainer module of transformer
  def compute_loss(self, model, inputs, return_outputs=False):

    outputs = model(**inputs)    # feeding input to model to have output
    logits = outputs.get("logits") # from this output we will calculate logits, which we will compare with label to compute loss
    labels = inputs.get("labels")  # extract labels
    loss_func = nn.CrossEntropyLoss(weight = class_weights)
    loss = loss_func(logits,labels)

    return (loss,outputs) if  return_outputs else loss

# Now we have to define Model , training dataset , Hyperparametres, Metrics
# initialize model
from transformers import AutoModelForSequenceClassification  # as we are using text classification , we use Sequence classification
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=6,id2label=id2label,label2id=label2id)

from sklearn.metrics import *

def compute_metrics(pred):
  labels = pred.label_ids   # extract
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels,preds,average="weighted")
  return {"f1":f1}

from transformers.training_args_tf import TrainingArguments
# Hyperparametre for model

from transformers import TrainingArguments

batch_size = 64
# log the training loss at each epoch
logging_steps = len(emotion_df['train']) // batch_size  #(16000//64)
output_dir = "/content/drive/MyDrive/Endpoint/output"                # which will save checkpoint of model,tokenizer, config details
training_args  = TrainingArguments(output_dir=output_dir,
                                   num_train_epochs=5,
                                   learning_rate=2e-5,
                                   per_device_train_batch_size=batch_size,
                                   per_device_eval_batch_size=batch_size,
                                   weight_decay=0.01,
                                   evaluation_strategy='epoch',
                                   logging_steps=logging_steps
                                  )

trainer = WeightedLossTrainer(model=model,
                              args=training_args,
                              compute_metrics=compute_metrics,
                              train_dataset=emotion_df['train'],
                              eval_dataset=emotion_df['validation'],
                              tokenizer = tokenizer)

trainer.train()



test_predictions = trainer.predict(emotion_df['test'])  # giving test dataset

test_predictions

predicted_labels = test_predictions.predictions.argmax(axis=1)  # Extract the predicted labels
predicted_labels

